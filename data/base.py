"""
This file defines a base class `DatasetBase`.

It divides the samples into different buckets accordint to their length.
"""

from abc import ABC, abstractmethod
from datasets import load_from_disk
import numpy as np
import os
import pandas as pd
import torch
from torch import stack, tensor
from transformers import AutoTokenizer

# We divide the dataset into buckets based on the length of the context.
# This helps in efficient batching during training.
BUCKET_SIZE = [0, 256, 512, 1024, 2048, 4096, 8192, 16384]
# Due to different lengths of the buckets, we adjust the batch size accordingly
# to prevent Out of Memory (OOM) errors.
BUCKET_BATCH_SIZE = [8, 8, 8, 4, 4, 1, 1]
KEYS_FOR_TRAIN = ["input_ids", "chunk_ids", "cross_attention_mask", "labels"]
CPU_NUM = os.cpu_count()
HF_HOME = os.getenv('HF_HOME', '~/.cache/huggingface')
CACHE_DIR = HF_HOME + '/bucket_cache'
NUM_INSTANCES_PER_FILE = 1 << 20

def collate_fn(batch):
    return {
        key: stack([tensor(item[key]) for item in batch]) for key in KEYS_FOR_TRAIN
    }

def pad_tokens(example, chunk_length, input_length, label_column, pad_token_id=128004):
    input_id = example['input_ids']
    label = example[label_column]
    chunk_id = example['chunk_ids']
    mask = example['cross_attention_mask']
    # Concatenate input and label
    l = len(input_id)
    input_id = np.concatenate([input_id, label])
    label = np.pad(label, (l - 1, 0), constant_values=-100)
    # Truncate
    input_id = input_id[:input_length]
    label = label[:input_length]
    # Pad
    return pd.Series([
        np.pad(input_id, (0, input_length-len(input_id)), constant_values=pad_token_id),
        np.pad(chunk_id, (0, chunk_length-len(chunk_id)), constant_values=pad_token_id),
        np.pad(mask, (0, chunk_length-len(mask)), constant_values=0),
        np.pad(label, (0, input_length-len(label)), constant_values=-100)
    ], index=KEYS_FOR_TRAIN)

class DatasetBase(ABC):
    def __init__(self, model_name, split="train", max_input_length=512):
        self.model_name = model_name
        self.max_input_length = max_input_length
        self._init_tokenizer()
        cached_path = HF_HOME + f'/datasets/{self.name}_{model_name.replace('/', '_')}'
        if split == "train" and os.path.exists(cached_path):
            # Load custom dataset. Its answer is generated by the backbone model.
            self.data = load_from_disk(cached_path)
        else:
            self._init_data(split)

    @abstractmethod
    def _init_data(self, split):
        raise NotImplementedError("Tokenization must be implemented.")
        
    def _init_tokenizer(self):
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            # add new pad_token
            self.tokenizer.pad_token = '<PAD>'
            self.tokenizer.pad_token_id = 128004

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

    def bucketing(self, local_rank, world_size):
        num_files = 0
        # Only one rank handles the dataset
        if local_rank == 0 or world_size == 1:
            bucket_files = []
            os.makedirs(CACHE_DIR, exist_ok=True)
            df = self.data if isinstance(self.data, pd.DataFrame) else self.data.to_pandas()
            df['bucket'] = pd.cut(df['token_count'], bins=BUCKET_SIZE,
                                labels=range(len(BUCKET_SIZE) - 1), ordered=False)
            label_column = self.model_name if self.model_name in df else 'labels'
            for i, group_df in df.groupby('bucket', observed=True):
                bsz = BUCKET_BATCH_SIZE[i]
                # Shuffle
                group_df = group_df.sample(frac=1, random_state=42)
                # Divide the DataFrame if it is too large
                for start in range(0, len(group_df), NUM_INSTANCES_PER_FILE):
                    cache_file = os.path.join(CACHE_DIR, f"bucket_{num_files}.parquet")
                    num_files += 1

                    # Delete cache
                    if os.path.exists(cache_file):
                        os.remove(cache_file)

                    df = group_df.iloc[start: start + NUM_INSTANCES_PER_FILE]
                    max_length = df['token_count'].max()
                    df = df.apply(pad_tokens, axis=1, result_type='expand',
                                            args=(max_length, self.max_input_length,
                                            label_column, self.tokenizer.pad_token_id))
                    df.to_parquet(cache_file, index=False)
                    bucket_files.append((bsz, cache_file))

        # Synchronize
        if world_size > 1:
            # First broadcast the number of files
            n = tensor(num_files, dtype=torch.long, device='cuda')
            torch.distributed.broadcast(n, src=0)
            if local_rank != 0:
                bucket_files = [None] * n.item()

            # Then broadcast the file list
            torch.distributed.broadcast_object_list(bucket_files, src=0)
        
        return bucket_files
    
    @classmethod
    def scorer(self, example, method):
        score = 0.
        prediction = example[f'output_{method}']
        ground_truths = example['answers']
        if self.name in ["trec", "triviaqa", "samsum", "lsht"]:
            prediction = prediction.lstrip('\n').split('\n')[0]
        for ground_truth in ground_truths:
            score = max(score, self.metric(prediction, ground_truth))
        return {f'score_{method}': score}