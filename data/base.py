"""
This file defines a base class `DatasetBase`.

It divides the samples into different buckets accordint to their length.
"""

from datasets import load_from_disk
import numpy as np
import os
import pandas as pd
import torch
from torch import stack, tensor
from transformers import AutoTokenizer

# We divide the dataset into buckets based on the length of the context.
# This helps in efficient batching during training.
BUCKET_SIZE = [0, 256, 512, 1024, 2048, 4096, 8192, 16384]
# Due to different lengths of the buckets, we adjust the batch size accordingly
# to prevent Out of Memory (OOM) errors.
BUCKET_BATCH_SIZE = [8, 8, 8, 8, 4, 2, 1]
KEYS_FOR_TRAIN = ["input_ids", "chunk_ids", "cross_attention_mask", "labels"]
CPU_NUM = os.cpu_count()
HF_HOME = os.getenv('HF_HOME', '~/.cache/huggingface')
CACHE_DIR = HF_HOME + '/bucket_cache'

# def assign_bucket(example):
#     for i, s in enumerate(BUCKET_SIZE):
#         if 0 < example['token_count'] <= s:
#             return {'bucket': i}
#     # drop extra-long samples
#     return {'bucket': None}

def collate_fn(batch):
    return {
        key: stack([tensor(item[key]) for item in batch]) for key in KEYS_FOR_TRAIN
    }

# def pad_tokens(batch, chunk_length, input_length, label_column, pad_token_id=128004):
#     input_ids = []
#     labels = []
#     for input_id, label in zip(batch['input_ids'], batch[label_column]):
#         l = len(input_id)
#         # concatenate input and label
#         input_id += label
#         label = [-100] * (l - 1) + label
#         # truncate
#         input_id = input_id[:input_length]
#         label = label[:input_length]
#         # pad
#         input_ids.append(input_id + [pad_token_id]*(input_length-len(input_id)))
#         labels.append(label + [-100]*(input_length-len(label)))

#     return {
#         'input_ids': input_ids,
#         'chunk_ids': [chunk_ids + [pad_token_id]*(chunk_length-len(chunk_ids))
#             for chunk_ids in batch['chunk_ids']],
#         'cross_attention_mask': [mask + [0]*(chunk_length-len(mask))
#             for mask in batch['cross_attention_mask']],
#         'labels': labels
#     }

def pad_tokens(example, chunk_length, input_length, label_column, pad_token_id=128004):
    input_id = example['input_ids']
    label = example[label_column]
    chunk_id = example['chunk_ids']
    mask = example['cross_attention_mask']
    # Concatenate input and label
    l = len(input_id)
    input_id = np.concatenate([input_id, label])
    label = np.pad(label, (l - 1, 0), constant_values=-100)
    # Truncate
    input_id = input_id[:input_length]
    label = label[:input_length]
    # Pad
    return pd.Series([
        np.pad(input_id, (0, input_length-len(input_id)), constant_values=pad_token_id),
        np.pad(chunk_id, (0, chunk_length-len(chunk_id)), constant_values=pad_token_id),
        np.pad(mask, (0, chunk_length-len(mask)), constant_values=0),
        np.pad(label, (0, input_length-len(label)), constant_values=-100)
    ], index=KEYS_FOR_TRAIN)

class DatasetBase:
    def __init__(self, model_name, split="train", max_input_length=512):
        self.model_name = model_name
        self.max_input_length = max_input_length
        self._init_tokenizer()
        cached_path = HF_HOME + f'/datasets/{self.name}_{model_name.replace('/', '_')}'
        if split == "train" and os.path.exists(cached_path):
            # Load custom dataset. Its answer is generated by the backbone model.
            self.data = load_from_disk(cached_path)
        else:
            self._init_data(split)

    def _init_data(self, split):
        raise NotImplementedError("Tokenization must be implemented.")
        
    def _init_tokenizer(self):
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            # add new pad_token
            self.tokenizer.pad_token = '<PAD>'
            self.tokenizer.pad_token_id = 128004

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

    def bucketing(self, local_rank, world_size):
        # self.data = self.data.map(assign_bucket)
        # grouped_dataloader = []
        # for i, bsz in enumerate(BUCKET_BATCH_SIZE):
        #     filtered_data = self.data.filter(lambda x: x['bucket'] == i)
        #     if len(filtered_data) > 0:
        #         max_length = max(filtered_data['token_count'])
        #         padded_data = filtered_data.map(pad_tokens, batched=True,
        #                         fn_kwargs={'chunk_length': max_length,
        #                         'input_length': self.max_input_length,
        #                         'pad_token_id': self.tokenizer.pad_token_id})
        #         grouped_dataloader.append((bsz, 
        #             DataLoader(padded_data, batch_size=bsz, collate_fn=collate_fn,
        #                         shuffle=True, num_workers=CPU_NUM)
        #         ))
        
        # return grouped_dataloader
        bucket_files = [os.path.join(CACHE_DIR, f"bucket_{i}.parquet")
                        for i in range(len(BUCKET_BATCH_SIZE))]
        # Only one process handles the dataset
        if local_rank == 0 or world_size == 1:
            os.makedirs(CACHE_DIR, exist_ok=True)
            # Delete cache
            for file in bucket_files:
                if os.path.exists(file):
                    os.remove(file)

            df = self.data if isinstance(self.data, pd.DataFrame) else self.data.to_pandas()
            df['bucket'] = pd.cut(df['token_count'], bins=BUCKET_SIZE,
                                labels=range(len(BUCKET_SIZE) - 1), ordered=False)
            label_column = self.model_name if self.model_name in df else 'labels'
            for i, group_df in df.groupby('bucket', observed=True):
                max_length = group_df['token_count'].max()
                padded_df = group_df.apply(pad_tokens, axis=1, result_type='expand',
                                        args=(max_length, self.max_input_length,
                                        label_column, self.tokenizer.pad_token_id))
                padded_df.to_parquet(bucket_files[i], index=False)

        # Synchronize
        if world_size > 1:
            torch.distributed.barrier()
        
        return zip(BUCKET_BATCH_SIZE, bucket_files)
    
    def scorer(self, example, method):
        score = 0.
        prediction = example[f'output_{method}']
        ground_truths = example['answers']
        if self.name in ["trec", "triviaqa", "samsum", "lsht"]:
            prediction = prediction.lstrip('\n').split('\n')[0]
        for ground_truth in ground_truths:
            score = max(score, self.metric(prediction, ground_truth))
        return {f'score_{method}': score}